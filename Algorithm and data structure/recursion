A recursive function should have the following properties so that it does not result in an infinite loop:

1. A simple base case (or cases) â€” a terminating scenario that does not use recursion to produce an answer.
2. A set of rules, also known as recurrence relation that reduces all other cases towards the base case.

In this article we will look closer into the duplicate calculations problem that could happen with recursion.
We will then propose a common technique called memoization that can be used to avoid this problem.

To eliminate the duplicate calculation in the above case, as many of you would have figured out, 
one of the ideas would be to store the intermediate results in the cache so that we could reuse them later without re-calculation.

Memoization is an optimization technique used primarily to speed up computer programs by storing the results of expensive function
calls and returning the cached result when the same inputs occur again. 

Back to our Fibonacci function F(n). We could use a hash table to keep track of the result of each F(n) with n as the key. The hash
table serves as a cache that saves us from duplicate calculations. The memoization technique is a good example that demonstrates 
how one can reduce compute time in exchange for some additional space.

import java.util.HashMap;

public class Main {

  HashMap<Integer, Integer> cache = new HashMap<Integer, Integer>();

  private int fib(int N) {
    if (cache.containsKey(N)) {
      return cache.get(N);
    }
    int result;
    if (N < 2) {
      result = N;
    } else {
      result = fib(N-1) + fib(N-2);
    }
    // keep the result in cache.
    cache.put(N, result);
    return result;
  }
}
................................................................................
Time Complexity - Recursion
Given a recursion algorithm, its time complexity O(T) is typically the product of the number of recursion invocations (denoted as R)
and the time complexity of calculation (denoted as O(s)) that incurs along with each recursion call:

O(T) = R * O(s)

Fibonacci number: f(n) = f(n-1) + f(n - 2)
The execution tree of a recursive function would form an n-ary tree, with n as the number of times recursion appears in the recurrence relation.

Space Complexity - Recursion

Recursion Related Space + Non-Recursion Related Space
Non-Recursion Related Space: which typically includes the space (normally in heap) that is allocated for the global variables.
...............................................................................

Tail recursion is a recursion where the recursive call is the final instruction in the recursion function. And there should be 
only one recursive call in the function.

public class Main {
    
  private static int helper_non_tail_recursion(int start, int [] ls) {
    if (start >= ls.length) {
      return 0;
    }
    // not a tail recursion because it does some computation after the recursive call returned.
    return ls[start] + helper_non_tail_recursion(start+1, ls);
  }

  public static int sum_non_tail_recursion(int [] ls) {
    if (ls == null || ls.length == 0) {
      return 0;
    }
    return helper_non_tail_recursion(0, ls);
  }

  //---------------------------------------------

  private static int helper_tail_recursion(int start, int [] ls, int acc) {
    if (start >= ls.length) {
      return acc;
    }
    // this is a tail recursion because the final instruction is the recursive call.
    return helper_tail_recursion(start+1, ls, acc+ls[start]);
  }
    
  public static int sum_tail_recursion(int [] ls) {
    if (ls == null || ls.length == 0) {
      return 0;
    }
    return helper_tail_recursion(0, ls, 0);
  }
}

The benefit of having tail recursion is that it could avoid the accumulation of stack overheads during the recursive calls, 
since the system could reuse a fixed amount space in the stack for each recursive call. 

A tail recursion function can be executed as non-tail-recursion functions, i.e. with piles of call stacks, without impact on the result. Often, the compiler recognizes tail recursion pattern, and optimizes its execution. However, not all programming languages support this optimization. For instance, C, C++ support the optimization of tail recursion functions. On the other hand, Java and Python do not support tail recursion optimization.
.....................................................................................................

1. write down the recurrence relationship.

2. Whenever possible, apply memoization.

3. When stack overflows, tail recursion might come to help. 


.......................................................................................................

*******************************************************************************************************

.......................................................................................................

Recursion II

In particular, we will cover some paradigms that are often applied together with the recursion to solve some problems.
1. Divide and Conquer
2. Backtracking
3. master theorem

A divide-and-conquer algorithm works by recursively breaking the problem down into two or more subproblems of the same
or related type, until these subproblems become simple enough to be solved directly. Then one combines the results of 
subproblems to form the final solution.

def divide_and_conquer( S ):
    # (1). Divide the problem into a set of subproblems.
    [S1, S2, ... Sn] = divide(S)

    # (2). Solve the subproblem recursively,
    #   obtain the results of subproblems as [R1, R2... Rn].
    rets = [divide_and_conquer(Si) for Si in [S1, S2, ... Sn]]
    [R1, R2,... Rn] = rets

    # (3). combine the results from the subproblems.
    #   and return the combined result.
    return combine([R1, R2,... Rn])

1. Merge Sort
There are two approaches to implement the merge sort algorithm: top down or bottom up. Here, we will explain the top
down approach as it can be implemented naturally using recursion.

*In the first step, we divide the list into two sublists.  (Divide)
*Then in the next step, we recursively sort the sublists in the previous step.  (Conquer)
*Finally we merge the sorted sublists in the above step repeatedly to obtain the final list of sorted elements.  (Combine)
(Merging two sorted lists can be done in linear time complexity O(N), where N is the total lengths of the two lists to merge.)

Top-down Approach
import java.util.Arrays;
public class Solution {
    
    public int [] merge_sort(int [] input) {
      if (input.length <= 1) {
        return input;
      }
      int pivot = input.length / 2;
      int [] left_list = merge_sort(Arrays.copyOfRange(input, 0, pivot));
      int [] right_list = merge_sort(Arrays.copyOfRange(input, pivot, input.length));
      return merge(left_list, right_list);
    }
    
    public int [] merge(int [] left_list, int [] right_list) {
      int [] ret = new int[left_list.length + right_list.length];
      int left_cursor = 0, right_cursor = 0, ret_cursor = 0;

      while (left_cursor < left_list.length && 
             right_cursor < right_list.length) {
        if (left_list[left_cursor] < right_list[right_cursor]) {
          ret[ret_cursor++] = left_list[left_cursor++];
        } else {
          ret[ret_cursor++] = right_list[right_cursor++];
        }
      }
      // append what is remain the above lists
      while (left_cursor < left_list.length) {
        ret[ret_cursor++] = left_list[left_cursor++];
      }
      while (right_cursor < right_list.length) {
        ret[ret_cursor++] = right_list[right_cursor++];
      }  
      return ret;
    }
}

Bottom-up Approach
In the bottom up approach, we divide the list into sublists of a single element at the beginning. 
Each of the sublists is then sorted already. Then from this point on, we merge the sublists two 
at a time until a single list remains.

Complexity
The overall time complexity of the merge sort algorithm is O(NlogN), where N is the length of the 
input list. To calculate the complexity, we break it down to the following steps:

1. We recursively divide the input list into two sublists, until a sublist with single element remains. 
This dividing step computes the midpoint of each of the sublists, which takes O(1) time. This step is
repeated N times until a single element remains, therefore the total time complexity is O(N).
 
2. Then, we repetitively merge the sublists, until one single list remains. As shown in the recursion tree, 
there are a total of N elements on each level. Therefore, it takes O(N) time for the merging process to 
complete on each level. And since there are a total of logN levels, the overall complexity of the merge
process is O(NlogN).

2. Quick Sort
Quick sort algorithm can be implemented in three steps, namely dividing the problem, solving the subproblems and combining the results of subproblems.
In detail, given a list of values to sort, the quick sort algorithm works in the following steps:

1. First, it selects a value from the list, which serves as a pivot value to divide the list into two sublists. One sublist contains all the values that are less than the pivot value, while the other sublist contains the values that are greater than or equal to the pivot value. This process is also called partitioning. The strategy of choosing a pivot value can vary. Typically, one can choose the first element in the list as the pivot, or randomly pick an element from the list.
2. After the partitioning process, the original list is then reduced into two smaller sublists. We then recursively sort the two sublists.
3. After the partitioning process, we are sure that all elements in one sublist are less or equal than any element in another sublist. Therefore, we can simply concatenate the two sorted sublists that we obtain in step [2] to obtain the final sorted list.

The base cases of the recursion in step [2] are either when the input list is empty or the empty list contains only a single element. In either case, the input list can be considered as sorted already.
As one can see, the essential idea of the quick sort algorithm is the partitioning process, which elegantly reduces the problems into smaller scale and meanwhile moves towards the final solution, i.e. after each partitioning, the overall values become more ordered.

public class Solution {
  public void quickSort(int [] lst) {
   /* Sorts an array in the ascending order in O(n log n) time */
    int n = lst.length;
    qSort(lst, 0, n - 1);
  }
  private void qSort(int [] lst, int lo, int hi) {
    if (lo < hi) {
      int p = partition(lst, lo, hi);
      qSort(lst, lo, p - 1);
      qSort(lst, p + 1, hi);
    }
  }
  private int partition(int [] lst, int lo, int hi) {
    /*
      Picks the last element hi as a pivot
      and returns the index of pivot value in the sorted array */
    int pivot = lst[hi];
    int i = lo;
    for (int j = lo; j < hi; ++j) {
      if (lst[j] < pivot) {
        int tmp = lst[i];
        lst[i] = lst[j];
        lst[j] = tmp;
        i++;
      }
    }
    int tmp = lst[i];
    lst[i] = lst[hi];
    lst[hi] = tmp;
    return i;
  }
}

..................
complexity
...................
Depending on the pivot values, the time complexity of the quick sort algorithm can vary from O(Nlog_2{N}) in the
best case and O(N^2)in the worst case, with N as the length of the list.

In the best case, if the pivot value happens to be median value of the list, then at each partition the list
would be divided into two sublists of equal size. At the end, we actually construct a balanced binary search tree
 out of the list. One can infer that the height of the tree would be log_2{N},and at each level of the tree the
input list would be scanned once with the complexity O(N) due to the partitioning process. As a result, the overall
time complexity of the algorithm in this case would be O(Nlog_2{N}).

While in the worst case, if the pivot value happens to be the extreme value of the list, i.e. either the smallest
or the biggest element in the list, then at each partition we end up with only one single sublist (i.e. the other
sublist is empty). The reduction of the problem still works, but at a rather slow pace, i.e. one element at a time.
The partitioning would then occur N times, and each time the partitioning scans at most N elements. Therefore, the
 overall time complexity of the quick sort algorithm in this case would be O(N^2).Actually, in this case, the quick
sort algorithm ends up to be exactly as the insertion sort.

On average, as proved mathematically, the time complexity of quick sort is O(Nlog_2{N})

................
Master Theorem
................

Master Theorem to quickly calculate the time complexity of many recursion algorithms.
Master Theorem, also known as Master Method, provides asymptotic analysis (i.e. the time complexity) for many of the
 recursion algorithms that follow the pattern of divide-and-conquer.
 function dac( n ):
   if n < k:  // k: some constant
     Solve "n" directly without recursion
   else:
     [1]. divide the problem "n" into "b" subproblems of equal size.
       - then the size of each subproblem would be "n/b"
     [2]. call the function "dac()" recursively "a" times on the subproblems
     [3]. combine the results from the subproblems
If we define the time complexity of the above recursion algorithm asT(n), then we can express it as follows:
                                            T(n)=aâ‹…T(b/n)+f(n)
for more detail: https://leetcode.com/explore/learn/card/recursion-ii/470/divide-and-conquer/2871/




