A recursive function should have the following properties so that it does not result in an infinite loop:

1. A simple base case (or cases) â€” a terminating scenario that does not use recursion to produce an answer.
2. A set of rules, also known as recurrence relation that reduces all other cases towards the base case.

In this article we will look closer into the duplicate calculations problem that could happen with recursion.
We will then propose a common technique called memoization that can be used to avoid this problem.

To eliminate the duplicate calculation in the above case, as many of you would have figured out, 
one of the ideas would be to store the intermediate results in the cache so that we could reuse them later without re-calculation.

Memoization is an optimization technique used primarily to speed up computer programs by storing the results of expensive function
calls and returning the cached result when the same inputs occur again. 

Back to our Fibonacci function F(n). We could use a hash table to keep track of the result of each F(n) with n as the key. The hash
table serves as a cache that saves us from duplicate calculations. The memoization technique is a good example that demonstrates 
how one can reduce compute time in exchange for some additional space.

import java.util.HashMap;

public class Main {

  HashMap<Integer, Integer> cache = new HashMap<Integer, Integer>();

  private int fib(int N) {
    if (cache.containsKey(N)) {
      return cache.get(N);
    }
    int result;
    if (N < 2) {
      result = N;
    } else {
      result = fib(N-1) + fib(N-2);
    }
    // keep the result in cache.
    cache.put(N, result);
    return result;
  }
}
................................................................................
Time Complexity - Recursion
Given a recursion algorithm, its time complexity O(T) is typically the product of the number of recursion invocations (denoted as R)
and the time complexity of calculation (denoted as O(s)) that incurs along with each recursion call:

O(T) = R * O(s)

Fibonacci number: f(n) = f(n-1) + f(n - 2)
The execution tree of a recursive function would form an n-ary tree, with n as the number of times recursion appears in the recurrence relation.

Space Complexity - Recursion

Recursion Related Space + Non-Recursion Related Space
Non-Recursion Related Space: which typically includes the space (normally in heap) that is allocated for the global variables.
...............................................................................

Tail recursion is a recursion where the recursive call is the final instruction in the recursion function. And there should be 
only one recursive call in the function.

public class Main {
    
  private static int helper_non_tail_recursion(int start, int [] ls) {
    if (start >= ls.length) {
      return 0;
    }
    // not a tail recursion because it does some computation after the recursive call returned.
    return ls[start] + helper_non_tail_recursion(start+1, ls);
  }

  public static int sum_non_tail_recursion(int [] ls) {
    if (ls == null || ls.length == 0) {
      return 0;
    }
    return helper_non_tail_recursion(0, ls);
  }

  //---------------------------------------------

  private static int helper_tail_recursion(int start, int [] ls, int acc) {
    if (start >= ls.length) {
      return acc;
    }
    // this is a tail recursion because the final instruction is the recursive call.
    return helper_tail_recursion(start+1, ls, acc+ls[start]);
  }
    
  public static int sum_tail_recursion(int [] ls) {
    if (ls == null || ls.length == 0) {
      return 0;
    }
    return helper_tail_recursion(0, ls, 0);
  }
}

The benefit of having tail recursion is that it could avoid the accumulation of stack overheads during the recursive calls, 
since the system could reuse a fixed amount space in the stack for each recursive call. 

A tail recursion function can be executed as non-tail-recursion functions, i.e. with piles of call stacks, without impact on the result. Often, the compiler recognizes tail recursion pattern, and optimizes its execution. However, not all programming languages support this optimization. For instance, C, C++ support the optimization of tail recursion functions. On the other hand, Java and Python do not support tail recursion optimization.
.....................................................................................................

1. write down the recurrence relationship.

2. Whenever possible, apply memoization.

3. When stack overflows, tail recursion might come to help. 


.......................................................................................................

*******************************************************************************************************

.......................................................................................................

Recursion II

In particular, we will cover some paradigms that are often applied together with the recursion to solve some problems.
1. Divide and Conquer
2. Backtracking
3. master theorem

A divide-and-conquer algorithm works by recursively breaking the problem down into two or more subproblems of the same
or related type, until these subproblems become simple enough to be solved directly. Then one combines the results of 
subproblems to form the final solution.

def divide_and_conquer( S ):
    # (1). Divide the problem into a set of subproblems.
    [S1, S2, ... Sn] = divide(S)

    # (2). Solve the subproblem recursively,
    #   obtain the results of subproblems as [R1, R2... Rn].
    rets = [divide_and_conquer(Si) for Si in [S1, S2, ... Sn]]
    [R1, R2,... Rn] = rets

    # (3). combine the results from the subproblems.
    #   and return the combined result.
    return combine([R1, R2,... Rn])

1. Merge Sort
There are two approaches to implement the merge sort algorithm: top down or bottom up. Here, we will explain the top
down approach as it can be implemented naturally using recursion.

*In the first step, we divide the list into two sublists.  (Divide)
*Then in the next step, we recursively sort the sublists in the previous step.  (Conquer)
*Finally we merge the sorted sublists in the above step repeatedly to obtain the final list of sorted elements.  (Combine)
(Merging two sorted lists can be done in linear time complexity O(N), where N is the total lengths of the two lists to merge.)

Top-down Approach
import java.util.Arrays;
public class Solution {
    
    public int [] merge_sort(int [] input) {
      if (input.length <= 1) {
        return input;
      }
      int pivot = input.length / 2;
      int [] left_list = merge_sort(Arrays.copyOfRange(input, 0, pivot));
      int [] right_list = merge_sort(Arrays.copyOfRange(input, pivot, input.length));
      return merge(left_list, right_list);
    }
    
    public int [] merge(int [] left_list, int [] right_list) {
      int [] ret = new int[left_list.length + right_list.length];
      int left_cursor = 0, right_cursor = 0, ret_cursor = 0;

      while (left_cursor < left_list.length && 
             right_cursor < right_list.length) {
        if (left_list[left_cursor] < right_list[right_cursor]) {
          ret[ret_cursor++] = left_list[left_cursor++];
        } else {
          ret[ret_cursor++] = right_list[right_cursor++];
        }
      }
      // append what is remain the above lists
      while (left_cursor < left_list.length) {
        ret[ret_cursor++] = left_list[left_cursor++];
      }
      while (right_cursor < right_list.length) {
        ret[ret_cursor++] = right_list[right_cursor++];
      }  
      return ret;
    }
}

Bottom-up Approach
In the bottom up approach, we divide the list into sublists of a single element at the beginning. 
Each of the sublists is then sorted already. Then from this point on, we merge the sublists two 
at a time until a single list remains.

Complexity
The overall time complexity of the merge sort algorithm is O(NlogN), where N is the length of the 
input list. To calculate the complexity, we break it down to the following steps:

1. We recursively divide the input list into two sublists, until a sublist with single element remains. 
This dividing step computes the midpoint of each of the sublists, which takes O(1) time. This step is
repeated N times until a single element remains, therefore the total time complexity is O(N).
 
2. Then, we repetitively merge the sublists, until one single list remains. As shown in the recursion tree, 
there are a total of N elements on each level. Therefore, it takes O(N) time for the merging process to 
complete on each level. And since there are a total of logN levels, the overall complexity of the merge
process is O(NlogN).





